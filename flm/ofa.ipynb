{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655d94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoConfig\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68c751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029bb5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, seq_len, d_model=768):\n",
    "        \"\"\"\n",
    "        Token Embedding layer for patched data.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels: 2 for RF fingerprint.\n",
    "            d_model (int): Dimension of the model: 768 for BERT_base.\n",
    "            seq_len (int): Length of the patched sequence.\n",
    "        \"\"\"\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, d_model, kernel_size=3, padding=1, bias=False)\n",
    "        self.linear = nn.Linear(seq_len, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size*n_patches=B*N, in_channels=C, seq_len=L)\n",
    "        x = self.conv(x)  # (B*N, d_model, L)\n",
    "        x = self.linear(x)  # (B*N, d_model, 1)\n",
    "        x = x.squeeze(-1)  # (B*N, d_model)\n",
    "        return x\n",
    "    \n",
    "# Patching and Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=2, patch_size=32, stride=16, d_model=768, dropout=0.1, norm=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels. 6 for IMU accelerometer+gyroscope.\n",
    "            patch_size (int): Size (length) of the patches.  \n",
    "            stride (int): Stride for patching.\n",
    "            d_model (int): Dimension of the model: 768 for BERT_base. 1024 for BERT_large.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.stride = stride\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Instance Normalization before patching\n",
    "        self.norm = norm\n",
    "        self.instance_norm = nn.InstanceNorm1d(num_features=in_channels, affine=True)\n",
    "\n",
    "        # embed each patch (token)\n",
    "        self.value_embedding = TokenEmbedding(in_channels, patch_size, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size=B, in_channels=C, total_seq_len)\n",
    "        batch_size, _, seq_len = x.size()\n",
    "\n",
    "        # apply instance normalization\n",
    "        if self.norm :\n",
    "            x = self.instance_norm(x)\n",
    "\n",
    "        # pad last value of stride length to the end\n",
    "        last_value = x[:, :, -1].unsqueeze(2).repeat(1, 1, self.stride)\n",
    "        x = torch.cat((x, last_value), dim=2)  # (batch_size, in_channels, seq_len + stride)\n",
    "\n",
    "        # patching\n",
    "        patches = []\n",
    "        for i in range(0, seq_len + self.stride, self.stride):\n",
    "            if i + self.patch_size <= seq_len + self.stride:\n",
    "                patch = x[:, :, i:i + self.patch_size]\n",
    "                patches.append(patch)\n",
    "        patches = torch.stack(patches, dim=3)  # (batch_size, in_channels, patch_size, num_patches)\n",
    "        patches = patches.permute(0, 3, 1, 2)  # (batch_size, num_patches, in_channels, patch_size)\n",
    "        # (B*N, C, L)\n",
    "        patches = patches.reshape(batch_size*patches.size(1), self.in_channels, self.patch_size)\n",
    "\n",
    "        # token embedding\n",
    "        embeddings = self.value_embedding(patches)  # (B*N, d_model)\n",
    "        embeddings = embeddings.view(batch_size, -1, self.d_model)  # (B, N, d_model)\n",
    "        # print('emb:',embeddings.size())\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=256):\n",
    "        \"\"\"\n",
    "        Learnable Positional Embedding for BERT.\n",
    "        Args:\n",
    "            d_model (int): Dimension of the model (e.g., 768 for BERT_base).\n",
    "            max_len (int): Maximum sequence length (number of patches).\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # learnable positional embeddings\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        # initialize positional embeddings\n",
    "        nn.init.normal_(self.positional_embedding, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, num_patches, d_model)\n",
    "        batch_size, num_patches, d_model = x.size()\n",
    "        # add positional embeddings\n",
    "        positional_embeddings = self.positional_embedding[:, :num_patches, :]\n",
    "        x = x + positional_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff01d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedLLM(nn.Module):\n",
    "    def __init__(self, patch_embed, model_name=\"bert-base-uncased\", \n",
    "                 max_len=512, dropout=0.1, num_classes=6):\n",
    "        \"\"\"\n",
    "        BERT model with pre-trained weights, fine-tuning only positional embeddings and layer normalization.\n",
    "        Args:\n",
    "            patch_embed (nn.Module): Patch embedding module.\n",
    "            model_name (str): Name of the pre-trained BERT model.\n",
    "            max_len (int): Maximum sequence length for positional embeddings.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(FineTunedLLM, self).__init__()\n",
    "        self.patch_embed = patch_embed\n",
    "\n",
    "        # Load pre-trained BERT model\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.llm = AutoModel.from_pretrained(model_name)\n",
    "        d_model = self.config.hidden_size # 768 for BERT_base, 1024 for BERT_large\n",
    "\n",
    "        # Use custom PositionalEncoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_len)\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        # Freeze all params\n",
    "        for p in self.llm.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Unfreeze LayerNorm or RMSNorm layers\n",
    "        for name, module in self.llm.named_modules():\n",
    "            if isinstance(module, (nn.LayerNorm)):\n",
    "                if hasattr(module, 'weight') and module.weight is not None:\n",
    "                    module.weight.requires_grad = True\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    module.bias.requires_grad = True\n",
    "\n",
    "        # Don't use input_ids embedding\n",
    "        # Important: do NOT delete them â€” just avoid using them in forward\n",
    "        # Enable PE to be trained\n",
    "        self.positional_encoding.positional_embedding.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, in_channels, total_seq_len)\n",
    "        # Patch embedding\n",
    "        embeddings = self.patch_embed(x) # (batch_size, num_patches, d_model)\n",
    "        # Apply positional encoding\n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "\n",
    "        # Pass through LLM encoder (skip token embeddings etc.)\n",
    "        if hasattr(self.llm, \"encoder\"):  # BERT-style\n",
    "            last_hidden_state = self.llm.encoder(\n",
    "                hidden_states=embeddings,\n",
    "                attention_mask=None,\n",
    "                return_dict=True\n",
    "            ).last_hidden_state # (batch_size, num_patches, d_model)\n",
    "        else:\n",
    "            # GPT / LLaMA etc.\n",
    "            last_hidden_state = self.llm(inputs_embeds=embeddings, attention_mask=None).last_hidden_state\n",
    "\n",
    "        # Use the mean of the last hidden state as the representation\n",
    "        representation = last_hidden_state.mean(dim=1) # (batch_size, d_model)\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.classifier(representation)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f00bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97301e10",
   "metadata": {},
   "source": [
    "### HHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07048c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9166, 6, 120)\n",
      "(9166, 120, 3)\n"
     ]
    }
   ],
   "source": [
    "# load hhar data as example\n",
    "hhar_data = np.load('../dataset/hhar/data_20_120.npy')\n",
    "hhar_label = np.load('../dataset/hhar/label_20_120.npy')\n",
    "\n",
    "# transpose from (N, 120, 6) to (N, 6, 120)\n",
    "hhar_data = hhar_data.transpose(0, 2, 1)\n",
    "\n",
    "print(hhar_data.shape) # (9166, 120, 6) 120 = sequence length, 6 = 3 axis * 2 (acc + gyro)\n",
    "print(hhar_label.shape) # (9166, 120, 3) ('user', 'model', 'gt') gt = (bike, sit, stairsdown, stairsup, stand, walk): 6 classes\n",
    "\n",
    "# extract the last dimension of the label (gt) to form a 1D array (9166,)\n",
    "hhar_label = hhar_label[:, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7eca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, label, seed, test_size=0.4, val_size=0.5):\n",
    "    set_seed(seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data, label, test_size=test_size, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=val_size, random_state=seed)\n",
    "    \n",
    "    train_set = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "    val_set = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    test_set = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "set_seed(3431)\n",
    "train_loader, val_loader, test_loader = train_val_test_split(hhar_data, hhar_label, seed=3431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa25eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/aul/homes/tzhao010/anaconda3/envs/zty/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FineTunedLLM(\n",
       "  (patch_embed): PatchEmbedding(\n",
       "    (instance_norm): InstanceNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (conv): Conv1d(6, 768, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (linear): Linear(in_features=8, out_features=1, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (llm): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(3431)\n",
    "# load the model to get the output embedding as the pseudo label\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "patch_size = 8\n",
    "stride = 4\n",
    "d_model = 768 # BERT base model dimension\n",
    "norm = False  # Use Instance Normalization before patching\n",
    "\n",
    "patch_embed = PatchEmbedding(in_channels=6, patch_size=patch_size, stride=stride, d_model=d_model, norm=norm)\n",
    "model = FineTunedLLM(patch_embed, model_name=\"gpt2\", max_len=256, dropout=0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63bd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "                device, num_epochs=50, save_path='best_model.pth', norm=False):\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Classification loss\n",
    "            cls_loss = criterion(outputs, labels)\n",
    "\n",
    "            cls_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += cls_loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_loss /= train_total\n",
    "        train_accuracy = train_correct / train_total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.6f}')\n",
    "\n",
    "        # --- Validation ---\n",
    "        if epoch % 10 == 9:\n",
    "            print(f\"Epoch {epoch+1}\", '-' * 40)\n",
    "            eval_acc = evaluate_model(model, val_loader, device)\n",
    "            print('-' * 50)\n",
    "            if eval_acc > best_acc:\n",
    "                best_acc = eval_acc\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Best model saved with accuracy: {best_acc:.6f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    test_loss /= test_total\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.6f}')\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a42ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 1.5246, Accuracy: 0.398618\n",
      "Epoch [2/50] - Train Loss: 1.3529, Accuracy: 0.437170\n",
      "Epoch [3/50] - Train Loss: 1.1824, Accuracy: 0.524459\n",
      "Epoch [4/50] - Train Loss: 0.7914, Accuracy: 0.726314\n",
      "Epoch [5/50] - Train Loss: 0.5822, Accuracy: 0.812148\n",
      "Epoch [6/50] - Train Loss: 0.4448, Accuracy: 0.862339\n",
      "Epoch [7/50] - Train Loss: 0.2865, Accuracy: 0.907256\n",
      "Epoch [8/50] - Train Loss: 0.2150, Accuracy: 0.933806\n",
      "Epoch [9/50] - Train Loss: 0.1734, Accuracy: 0.948354\n",
      "Epoch [10/50] - Train Loss: 0.1560, Accuracy: 0.952173\n",
      "Epoch 10 ----------------------------------------\n",
      "Test Loss: 0.1140, Test Accuracy: 0.963448\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.963448\n",
      "Epoch [11/50] - Train Loss: 0.1358, Accuracy: 0.956901\n",
      "Epoch [12/50] - Train Loss: 0.1274, Accuracy: 0.961629\n",
      "Epoch [13/50] - Train Loss: 0.1146, Accuracy: 0.965630\n",
      "Epoch [14/50] - Train Loss: 0.1133, Accuracy: 0.966721\n",
      "Epoch [15/50] - Train Loss: 0.1011, Accuracy: 0.972904\n",
      "Epoch [16/50] - Train Loss: 0.0939, Accuracy: 0.972904\n",
      "Epoch [17/50] - Train Loss: 0.0926, Accuracy: 0.974177\n",
      "Epoch [18/50] - Train Loss: 0.0891, Accuracy: 0.973813\n",
      "Epoch [19/50] - Train Loss: 0.0870, Accuracy: 0.975814\n",
      "Epoch [20/50] - Train Loss: 0.0691, Accuracy: 0.981269\n",
      "Epoch 20 ----------------------------------------\n",
      "Test Loss: 0.0882, Test Accuracy: 0.973268\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.973268\n",
      "Epoch [21/50] - Train Loss: 0.0731, Accuracy: 0.977269\n",
      "Epoch [22/50] - Train Loss: 0.0763, Accuracy: 0.975632\n",
      "Epoch [23/50] - Train Loss: 0.0672, Accuracy: 0.979996\n",
      "Epoch [24/50] - Train Loss: 0.0661, Accuracy: 0.981087\n",
      "Epoch [25/50] - Train Loss: 0.0590, Accuracy: 0.983270\n",
      "Epoch [26/50] - Train Loss: 0.0654, Accuracy: 0.981815\n",
      "Epoch [27/50] - Train Loss: 0.0542, Accuracy: 0.985816\n",
      "Epoch [28/50] - Train Loss: 0.0541, Accuracy: 0.983452\n",
      "Epoch [29/50] - Train Loss: 0.0530, Accuracy: 0.982724\n",
      "Epoch [30/50] - Train Loss: 0.0517, Accuracy: 0.984906\n",
      "Epoch 30 ----------------------------------------\n",
      "Test Loss: 0.0829, Test Accuracy: 0.972177\n",
      "--------------------------------------------------\n",
      "Epoch [31/50] - Train Loss: 0.0457, Accuracy: 0.987452\n",
      "Epoch [32/50] - Train Loss: 0.0709, Accuracy: 0.979633\n",
      "Epoch [33/50] - Train Loss: 0.0439, Accuracy: 0.988362\n",
      "Epoch [34/50] - Train Loss: 0.0395, Accuracy: 0.987816\n",
      "Epoch [35/50] - Train Loss: 0.0482, Accuracy: 0.986543\n",
      "Epoch [36/50] - Train Loss: 0.0453, Accuracy: 0.986179\n",
      "Epoch [37/50] - Train Loss: 0.0440, Accuracy: 0.986543\n",
      "Epoch [38/50] - Train Loss: 0.0351, Accuracy: 0.989089\n",
      "Epoch [39/50] - Train Loss: 0.0432, Accuracy: 0.986543\n",
      "Epoch [40/50] - Train Loss: 0.0321, Accuracy: 0.989453\n",
      "Epoch 40 ----------------------------------------\n",
      "Test Loss: 0.0759, Test Accuracy: 0.980906\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.980906\n",
      "Epoch [41/50] - Train Loss: 0.0379, Accuracy: 0.987634\n",
      "Epoch [42/50] - Train Loss: 0.0332, Accuracy: 0.989453\n",
      "Epoch [43/50] - Train Loss: 0.0383, Accuracy: 0.987816\n",
      "Epoch [44/50] - Train Loss: 0.0322, Accuracy: 0.990180\n",
      "Epoch [45/50] - Train Loss: 0.0339, Accuracy: 0.988543\n",
      "Epoch [46/50] - Train Loss: 0.0282, Accuracy: 0.992180\n",
      "Epoch [47/50] - Train Loss: 0.0218, Accuracy: 0.993090\n",
      "Epoch [48/50] - Train Loss: 0.0376, Accuracy: 0.988362\n",
      "Epoch [49/50] - Train Loss: 0.0393, Accuracy: 0.986361\n",
      "Epoch [50/50] - Train Loss: 0.0250, Accuracy: 0.992726\n",
      "Epoch 50 ----------------------------------------\n",
      "Test Loss: 0.0529, Test Accuracy: 0.984179\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.984179\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "            device, num_epochs=50, save_path='hhar.pth', norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1b2574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0295, Test Accuracy: 0.994547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9945474372955289"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36843f0a",
   "metadata": {},
   "source": [
    "### UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e47201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2088, 120, 6)\n",
      "(2088, 120, 2)\n",
      "(2088,)\n",
      "{0.0: 355, 1.0: 318, 2.0: 254, 3.0: 362, 4.0: 398, 5.0: 401}\n"
     ]
    }
   ],
   "source": [
    "uci_data = np.load('../dataset/uci/data_20_120.npy') \n",
    "uci_label = np.load('../dataset/uci/label_20_120.npy')\n",
    "\n",
    "print(uci_data.shape) # (2088, 120, 6) 120 = sequence length, 6 = 3 axis * 2 (acc + gyro)\n",
    "print(uci_label.shape) # (2088, 120, 2) 6 classes (walk, upstairs, downstairs, sit, stand, lay), 30 participants\n",
    "\n",
    "# transpose from (N, 120, 6) to (N, 6, 120)\n",
    "uci_data = uci_data.transpose(0, 2, 1)\n",
    "\n",
    "# extract the first dimension of the label (gt) to form a 1D array (2088,)\n",
    "uci_label = uci_label[:, 0, 0]\n",
    "print(uci_label.shape)\n",
    "\n",
    "# statistics of the label\n",
    "# count the number of each class\n",
    "unique, counts = np.unique(uci_label, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6977807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/aul/homes/tzhao010/anaconda3/envs/zty/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 1.6101, Accuracy: 0.359425\n",
      "Epoch [2/50] - Train Loss: 1.1431, Accuracy: 0.514377\n",
      "Epoch [3/50] - Train Loss: 1.0505, Accuracy: 0.541534\n",
      "Epoch [4/50] - Train Loss: 0.9938, Accuracy: 0.550319\n",
      "Epoch [5/50] - Train Loss: 0.9530, Accuracy: 0.587859\n",
      "Epoch [6/50] - Train Loss: 0.9336, Accuracy: 0.599042\n",
      "Epoch [7/50] - Train Loss: 0.8434, Accuracy: 0.651757\n",
      "Epoch [8/50] - Train Loss: 0.7889, Accuracy: 0.678914\n",
      "Epoch [9/50] - Train Loss: 0.6750, Accuracy: 0.729233\n",
      "Epoch [10/50] - Train Loss: 0.6257, Accuracy: 0.736422\n",
      "Epoch 10 ----------------------------------------\n",
      "Test Loss: 0.6920, Test Accuracy: 0.679426\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.679426\n",
      "Epoch [11/50] - Train Loss: 0.5611, Accuracy: 0.790735\n",
      "Epoch [12/50] - Train Loss: 0.4507, Accuracy: 0.844249\n",
      "Epoch [13/50] - Train Loss: 0.3947, Accuracy: 0.866613\n",
      "Epoch [14/50] - Train Loss: 0.3470, Accuracy: 0.882588\n",
      "Epoch [15/50] - Train Loss: 0.3346, Accuracy: 0.875399\n",
      "Epoch [16/50] - Train Loss: 0.2849, Accuracy: 0.905751\n",
      "Epoch [17/50] - Train Loss: 0.2373, Accuracy: 0.917732\n",
      "Epoch [18/50] - Train Loss: 0.2377, Accuracy: 0.915335\n",
      "Epoch [19/50] - Train Loss: 0.2205, Accuracy: 0.912141\n",
      "Epoch [20/50] - Train Loss: 0.2108, Accuracy: 0.924121\n",
      "Epoch 20 ----------------------------------------\n",
      "Test Loss: 0.2054, Test Accuracy: 0.935407\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.935407\n",
      "Epoch [21/50] - Train Loss: 0.2033, Accuracy: 0.924920\n",
      "Epoch [22/50] - Train Loss: 0.1966, Accuracy: 0.929712\n",
      "Epoch [23/50] - Train Loss: 0.1868, Accuracy: 0.926518\n",
      "Epoch [24/50] - Train Loss: 0.1987, Accuracy: 0.921725\n",
      "Epoch [25/50] - Train Loss: 0.1692, Accuracy: 0.935304\n",
      "Epoch [26/50] - Train Loss: 0.1954, Accuracy: 0.936102\n",
      "Epoch [27/50] - Train Loss: 0.1739, Accuracy: 0.924920\n",
      "Epoch [28/50] - Train Loss: 0.1679, Accuracy: 0.930511\n",
      "Epoch [29/50] - Train Loss: 0.1730, Accuracy: 0.930511\n",
      "Epoch [30/50] - Train Loss: 0.1674, Accuracy: 0.933706\n",
      "Epoch 30 ----------------------------------------\n",
      "Test Loss: 0.2544, Test Accuracy: 0.909091\n",
      "--------------------------------------------------\n",
      "Epoch [31/50] - Train Loss: 0.1616, Accuracy: 0.936102\n",
      "Epoch [32/50] - Train Loss: 0.1614, Accuracy: 0.932109\n",
      "Epoch [33/50] - Train Loss: 0.1526, Accuracy: 0.937700\n",
      "Epoch [34/50] - Train Loss: 0.1504, Accuracy: 0.936102\n",
      "Epoch [35/50] - Train Loss: 0.1428, Accuracy: 0.940895\n",
      "Epoch [36/50] - Train Loss: 0.1517, Accuracy: 0.940895\n",
      "Epoch [37/50] - Train Loss: 0.1492, Accuracy: 0.944089\n",
      "Epoch [38/50] - Train Loss: 0.1453, Accuracy: 0.945687\n",
      "Epoch [39/50] - Train Loss: 0.1480, Accuracy: 0.939297\n",
      "Epoch [40/50] - Train Loss: 0.1547, Accuracy: 0.935304\n",
      "Epoch 40 ----------------------------------------\n",
      "Test Loss: 0.1830, Test Accuracy: 0.923445\n",
      "--------------------------------------------------\n",
      "Epoch [41/50] - Train Loss: 0.1447, Accuracy: 0.940096\n",
      "Epoch [42/50] - Train Loss: 0.1510, Accuracy: 0.939297\n",
      "Epoch [43/50] - Train Loss: 0.1519, Accuracy: 0.936901\n",
      "Epoch [44/50] - Train Loss: 0.1427, Accuracy: 0.942492\n",
      "Epoch [45/50] - Train Loss: 0.1406, Accuracy: 0.942492\n",
      "Epoch [46/50] - Train Loss: 0.1426, Accuracy: 0.937700\n",
      "Epoch [47/50] - Train Loss: 0.1417, Accuracy: 0.940096\n",
      "Epoch [48/50] - Train Loss: 0.1405, Accuracy: 0.947284\n",
      "Epoch [49/50] - Train Loss: 0.1300, Accuracy: 0.947284\n",
      "Epoch [50/50] - Train Loss: 0.1349, Accuracy: 0.948083\n",
      "Epoch 50 ----------------------------------------\n",
      "Test Loss: 0.1970, Test Accuracy: 0.928230\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "set_seed(3431)\n",
    "train_loader, val_loader, test_loader = train_val_test_split(uci_data, uci_label, seed=3431)\n",
    "\n",
    "patch_embed = PatchEmbedding(in_channels=6, patch_size=patch_size, stride=stride, d_model=d_model, norm=norm)\n",
    "model = FineTunedLLM(patch_embed, model_name=\"gpt2\", max_len=256, dropout=0.1)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "            device, num_epochs=50, save_path='uci.pth', norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e946002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1942, Test Accuracy: 0.911483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9114832535885168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751817ea",
   "metadata": {},
   "source": [
    "### Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "383948f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4534, 120, 6)\n",
      "(4534, 120, 2)\n",
      "(4534,)\n",
      "{0.0: 402, 1.0: 490, 2.0: 1105, 3.0: 999, 4.0: 1112, 5.0: 426}\n"
     ]
    }
   ],
   "source": [
    "# motion\n",
    "motion_data = np.load('../dataset/motion/data_20_120.npy')\n",
    "motion_label = np.load('../dataset/motion/label_20_120.npy')\n",
    "\n",
    "print(motion_data.shape) # (4534, 120, 6) 120 = sequence length, 6 = 3 axis * 2 (acc + gyro)\n",
    "print(motion_label.shape) # (4534, 120, 2) 6 classes: (stairdown, stairup, sit, stand, walk, jog) + 24 participants\n",
    "\n",
    "# Normalize\n",
    "# divide 9.8 for acceleration\n",
    "# motion_data[:, :, :3] /= 9.8\n",
    "\n",
    "# transpose from (N, 120, 6) to (N, 6, 120)\n",
    "motion_data = motion_data.transpose(0, 2, 1)\n",
    "\n",
    "# extract the first dimension of the label (gt) to form a 1D array (4534,)\n",
    "motion_label = motion_label[:, 0, 0]\n",
    "print(motion_label.shape)\n",
    "\n",
    "# statistics of the label\n",
    "# count the number of each class\n",
    "unique, counts = np.unique(motion_label, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1e03ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 1.4349, Accuracy: 0.454412\n",
      "Epoch [2/50] - Train Loss: 0.8997, Accuracy: 0.644853\n",
      "Epoch [3/50] - Train Loss: 0.5055, Accuracy: 0.823529\n",
      "Epoch [4/50] - Train Loss: 0.3303, Accuracy: 0.881618\n",
      "Epoch [5/50] - Train Loss: 0.2800, Accuracy: 0.911397\n",
      "Epoch [6/50] - Train Loss: 0.2284, Accuracy: 0.930147\n",
      "Epoch [7/50] - Train Loss: 0.2125, Accuracy: 0.931618\n",
      "Epoch [8/50] - Train Loss: 0.1934, Accuracy: 0.943750\n",
      "Epoch [9/50] - Train Loss: 0.1973, Accuracy: 0.938603\n",
      "Epoch [10/50] - Train Loss: 0.1835, Accuracy: 0.946691\n",
      "Epoch 10 ----------------------------------------\n",
      "Test Loss: 0.1881, Test Accuracy: 0.941566\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.941566\n",
      "Epoch [11/50] - Train Loss: 0.1592, Accuracy: 0.960294\n",
      "Epoch [12/50] - Train Loss: 0.1506, Accuracy: 0.959926\n",
      "Epoch [13/50] - Train Loss: 0.1500, Accuracy: 0.961397\n",
      "Epoch [14/50] - Train Loss: 0.1440, Accuracy: 0.958088\n",
      "Epoch [15/50] - Train Loss: 0.1436, Accuracy: 0.957353\n",
      "Epoch [16/50] - Train Loss: 0.1281, Accuracy: 0.966176\n",
      "Epoch [17/50] - Train Loss: 0.1140, Accuracy: 0.969853\n",
      "Epoch [18/50] - Train Loss: 0.1211, Accuracy: 0.965809\n",
      "Epoch [19/50] - Train Loss: 0.1153, Accuracy: 0.968015\n",
      "Epoch [20/50] - Train Loss: 0.1076, Accuracy: 0.971324\n",
      "Epoch 20 ----------------------------------------\n",
      "Test Loss: 0.1365, Test Accuracy: 0.959206\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.959206\n",
      "Epoch [21/50] - Train Loss: 0.1063, Accuracy: 0.968750\n",
      "Epoch [22/50] - Train Loss: 0.0950, Accuracy: 0.975000\n",
      "Epoch [23/50] - Train Loss: 0.0903, Accuracy: 0.973897\n",
      "Epoch [24/50] - Train Loss: 0.0889, Accuracy: 0.973897\n",
      "Epoch [25/50] - Train Loss: 0.0877, Accuracy: 0.975000\n",
      "Epoch [26/50] - Train Loss: 0.0853, Accuracy: 0.975000\n",
      "Epoch [27/50] - Train Loss: 0.0722, Accuracy: 0.979412\n",
      "Epoch [28/50] - Train Loss: 0.0649, Accuracy: 0.979044\n",
      "Epoch [29/50] - Train Loss: 0.0787, Accuracy: 0.975735\n",
      "Epoch [30/50] - Train Loss: 0.0601, Accuracy: 0.983456\n",
      "Epoch 30 ----------------------------------------\n",
      "Test Loss: 0.1194, Test Accuracy: 0.965821\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.965821\n",
      "Epoch [31/50] - Train Loss: 0.0597, Accuracy: 0.983456\n",
      "Epoch [32/50] - Train Loss: 0.0528, Accuracy: 0.984559\n",
      "Epoch [33/50] - Train Loss: 0.0444, Accuracy: 0.985662\n",
      "Epoch [34/50] - Train Loss: 0.0496, Accuracy: 0.986765\n",
      "Epoch [35/50] - Train Loss: 0.0518, Accuracy: 0.983456\n",
      "Epoch [36/50] - Train Loss: 0.0459, Accuracy: 0.984926\n",
      "Epoch [37/50] - Train Loss: 0.0460, Accuracy: 0.982353\n",
      "Epoch [38/50] - Train Loss: 0.0387, Accuracy: 0.985294\n",
      "Epoch [39/50] - Train Loss: 0.0334, Accuracy: 0.989338\n",
      "Epoch [40/50] - Train Loss: 0.0397, Accuracy: 0.989338\n",
      "Epoch 40 ----------------------------------------\n",
      "Test Loss: 0.1100, Test Accuracy: 0.972437\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.972437\n",
      "Epoch [41/50] - Train Loss: 0.0407, Accuracy: 0.986765\n",
      "Epoch [42/50] - Train Loss: 0.0366, Accuracy: 0.990074\n",
      "Epoch [43/50] - Train Loss: 0.0303, Accuracy: 0.989338\n",
      "Epoch [44/50] - Train Loss: 0.0247, Accuracy: 0.992647\n",
      "Epoch [45/50] - Train Loss: 0.0282, Accuracy: 0.993015\n",
      "Epoch [46/50] - Train Loss: 0.0262, Accuracy: 0.992647\n",
      "Epoch [47/50] - Train Loss: 0.0306, Accuracy: 0.990441\n",
      "Epoch [48/50] - Train Loss: 0.0294, Accuracy: 0.988603\n",
      "Epoch [49/50] - Train Loss: 0.0262, Accuracy: 0.989338\n",
      "Epoch [50/50] - Train Loss: 0.0216, Accuracy: 0.993015\n",
      "Epoch 50 ----------------------------------------\n",
      "Test Loss: 0.1257, Test Accuracy: 0.974642\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.974642\n"
     ]
    }
   ],
   "source": [
    "set_seed(3431)\n",
    "train_loader, val_loader, test_loader = train_val_test_split(motion_data, motion_label, seed=3431)\n",
    "patch_embed = PatchEmbedding(in_channels=6, patch_size=patch_size, stride=stride, d_model=d_model, norm=norm)\n",
    "model = FineTunedLLM(patch_embed, model_name=\"gpt2\", max_len=256, dropout=0.1)\n",
    "model.to(device)  \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "            device, num_epochs=50, save_path='motion.pth', norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cf8ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1163, Test Accuracy: 0.976847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9768467475192943"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5764965",
   "metadata": {},
   "source": [
    "### Shoaib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc2d733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 120, 9)\n",
      "(10500, 120, 3)\n",
      "(10500,)\n",
      "{0.0: 1650, 1.0: 1500, 2.0: 1500, 3.0: 1500, 4.0: 1500, 5.0: 1350, 6.0: 1500}\n"
     ]
    }
   ],
   "source": [
    "shoaib_data = np.load('../dataset/shoaib/data_20_120.npy')\n",
    "shoaib_label = np.load('../dataset/shoaib/label_20_120.npy')\n",
    "\n",
    "print(shoaib_data.shape) # (10500, 120, 9) 120 = sequence length, 9 = 3 axis * 3 (acc + gyro + mag)\n",
    "print(shoaib_label.shape) # (10500, 120, 3) 7 classes: (walking, sitting, standing, jogging, biking, upstairs, downstairs) + 10 participants\n",
    "\n",
    "# remove mag data (last 3 channels)\n",
    "shoaib_data = shoaib_data[:, :, :6]\n",
    "# transpose from (N, 120, 9) to (N, 9, 120)\n",
    "shoaib_data = shoaib_data.transpose(0, 2, 1)\n",
    "\n",
    "# extract the first dimension of the label (gt) to form a 1D array (10500,)\n",
    "shoaib_label = shoaib_label[:, 0, 0]\n",
    "print(shoaib_label.shape)\n",
    "\n",
    "# statistics of the label\n",
    "# count the number of each class\n",
    "unique, counts = np.unique(shoaib_label, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7e35f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 1.4924, Accuracy: 0.405556\n",
      "Epoch [2/50] - Train Loss: 0.8509, Accuracy: 0.692698\n",
      "Epoch [3/50] - Train Loss: 0.4280, Accuracy: 0.838730\n",
      "Epoch [4/50] - Train Loss: 0.2966, Accuracy: 0.890952\n",
      "Epoch [5/50] - Train Loss: 0.2309, Accuracy: 0.918254\n",
      "Epoch [6/50] - Train Loss: 0.2013, Accuracy: 0.924921\n",
      "Epoch [7/50] - Train Loss: 0.1692, Accuracy: 0.940159\n",
      "Epoch [8/50] - Train Loss: 0.1680, Accuracy: 0.937460\n",
      "Epoch [9/50] - Train Loss: 0.1460, Accuracy: 0.948571\n",
      "Epoch [10/50] - Train Loss: 0.1377, Accuracy: 0.950794\n",
      "Epoch 10 ----------------------------------------\n",
      "Test Loss: 0.1419, Test Accuracy: 0.950000\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.950000\n",
      "Epoch [11/50] - Train Loss: 0.1220, Accuracy: 0.958095\n",
      "Epoch [12/50] - Train Loss: 0.1169, Accuracy: 0.959683\n",
      "Epoch [13/50] - Train Loss: 0.1059, Accuracy: 0.961587\n",
      "Epoch [14/50] - Train Loss: 0.1093, Accuracy: 0.960159\n",
      "Epoch [15/50] - Train Loss: 0.0932, Accuracy: 0.966508\n",
      "Epoch [16/50] - Train Loss: 0.0952, Accuracy: 0.964762\n",
      "Epoch [17/50] - Train Loss: 0.0893, Accuracy: 0.969365\n",
      "Epoch [18/50] - Train Loss: 0.0811, Accuracy: 0.970159\n",
      "Epoch [19/50] - Train Loss: 0.0904, Accuracy: 0.966349\n",
      "Epoch [20/50] - Train Loss: 0.0803, Accuracy: 0.972857\n",
      "Epoch 20 ----------------------------------------\n",
      "Test Loss: 0.1048, Test Accuracy: 0.965238\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.965238\n",
      "Epoch [21/50] - Train Loss: 0.0770, Accuracy: 0.973492\n",
      "Epoch [22/50] - Train Loss: 0.0697, Accuracy: 0.975873\n",
      "Epoch [23/50] - Train Loss: 0.0629, Accuracy: 0.977619\n",
      "Epoch [24/50] - Train Loss: 0.0680, Accuracy: 0.976508\n",
      "Epoch [25/50] - Train Loss: 0.0611, Accuracy: 0.975714\n",
      "Epoch [26/50] - Train Loss: 0.0614, Accuracy: 0.979048\n",
      "Epoch [27/50] - Train Loss: 0.0591, Accuracy: 0.977619\n",
      "Epoch [28/50] - Train Loss: 0.0505, Accuracy: 0.980317\n",
      "Epoch [29/50] - Train Loss: 0.0699, Accuracy: 0.975238\n",
      "Epoch [30/50] - Train Loss: 0.0564, Accuracy: 0.980159\n",
      "Epoch 30 ----------------------------------------\n",
      "Test Loss: 0.1207, Test Accuracy: 0.967143\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.967143\n",
      "Epoch [31/50] - Train Loss: 0.0494, Accuracy: 0.982222\n",
      "Epoch [32/50] - Train Loss: 0.0534, Accuracy: 0.982222\n",
      "Epoch [33/50] - Train Loss: 0.0520, Accuracy: 0.981429\n",
      "Epoch [34/50] - Train Loss: 0.0438, Accuracy: 0.984603\n",
      "Epoch [35/50] - Train Loss: 0.0491, Accuracy: 0.983016\n",
      "Epoch [36/50] - Train Loss: 0.0464, Accuracy: 0.982063\n",
      "Epoch [37/50] - Train Loss: 0.0425, Accuracy: 0.984444\n",
      "Epoch [38/50] - Train Loss: 0.0498, Accuracy: 0.981905\n",
      "Epoch [39/50] - Train Loss: 0.0383, Accuracy: 0.986032\n",
      "Epoch [40/50] - Train Loss: 0.0470, Accuracy: 0.982540\n",
      "Epoch 40 ----------------------------------------\n",
      "Test Loss: 0.1686, Test Accuracy: 0.955714\n",
      "--------------------------------------------------\n",
      "Epoch [41/50] - Train Loss: 0.0573, Accuracy: 0.979048\n",
      "Epoch [42/50] - Train Loss: 0.0424, Accuracy: 0.986032\n",
      "Epoch [43/50] - Train Loss: 0.0419, Accuracy: 0.986667\n",
      "Epoch [44/50] - Train Loss: 0.0396, Accuracy: 0.985238\n",
      "Epoch [45/50] - Train Loss: 0.0364, Accuracy: 0.986825\n",
      "Epoch [46/50] - Train Loss: 0.0372, Accuracy: 0.985873\n",
      "Epoch [47/50] - Train Loss: 0.0363, Accuracy: 0.987937\n",
      "Epoch [48/50] - Train Loss: 0.0335, Accuracy: 0.988730\n",
      "Epoch [49/50] - Train Loss: 0.0455, Accuracy: 0.983651\n",
      "Epoch [50/50] - Train Loss: 0.0379, Accuracy: 0.986984\n",
      "Epoch 50 ----------------------------------------\n",
      "Test Loss: 0.1093, Test Accuracy: 0.969048\n",
      "--------------------------------------------------\n",
      "Best model saved with accuracy: 0.969048\n"
     ]
    }
   ],
   "source": [
    "set_seed(3431)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_test_split(shoaib_data, shoaib_label, seed=3431)\n",
    "patch_embed = PatchEmbedding(in_channels=6, patch_size=patch_size, stride=stride, d_model=d_model, norm=norm)\n",
    "model = FineTunedLLM(patch_embed, model_name=\"gpt2\", max_len=256, dropout=0.1, num_classes=7)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "            device, num_epochs=50, save_path='shoaib.pth', norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "182aad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1094, Test Accuracy: 0.964762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9647619047619047"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8fd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
