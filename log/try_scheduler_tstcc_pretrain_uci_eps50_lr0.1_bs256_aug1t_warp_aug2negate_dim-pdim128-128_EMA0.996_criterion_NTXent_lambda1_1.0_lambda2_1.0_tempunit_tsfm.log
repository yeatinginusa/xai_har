Namespace(EMA=0.996, aug1='t_warp', aug2='negate', backbone='ResNet18', batch_size=256, cases='random', criterion='NTXent', cuda=0, dataset='uci', device='Phones', framework='tstcc', lambda1=1.0, lambda2=1.0, len_sw=120, logdir='log/', lr=0.1, lr_cls=0.03, lr_mul=10.0, mmb_size=1024, model_name='try_scheduler_tstcc_pretrain_uci_eps50_lr0.1_bs256_aug1t_warp_aug2negate_dim-pdim128-128_EMA0.996_criterion_NTXent_lambda1_1.0_lambda2_1.0_tempunit_tsfm', n_class=6, n_epoch=50, n_feature=6, p=128, phid=128, plt=False, scheduler=True, split_ratio=0.2, target_domain='0', temp_unit='tsfm', weight_decay=0.0003)

Epoch : 0
Namespace(EMA=0.996, aug1='t_warp', aug2='negate', backbone='ResNet18', batch_size=256, cases='random', criterion='NTXent', cuda=0, dataset='uci', device='Phones', framework='tstcc', lambda1=1.0, lambda2=1.0, len_sw=120, logdir='log/', lr=0.1, lr_cls=0.03, lr_mul=10.0, mmb_size=1024, model_name='try_scheduler_tstcc_pretrain_uci_eps50_lr0.1_bs256_aug1t_warp_aug2negate_dim-pdim128-128_EMA0.996_criterion_NTXent_lambda1_1.0_lambda2_1.0_tempunit_tsfm', n_class=6, n_epoch=50, n_feature=6, p=128, phid=128, plt=False, scheduler=True, split_ratio=0.2, target_domain='0', temp_unit='tsfm', weight_decay=0.0003)

Epoch : 0
Namespace(EMA=0.996, aug1='t_warp', aug2='negate', backbone='ResNet18', batch_size=256, cases='random', criterion='NTXent', cuda=0, dataset='uci', device='Phones', framework='tstcc', lambda1=1.0, lambda2=1.0, len_sw=120, logdir='log/', lr=0.1, lr_cls=0.03, lr_mul=10.0, mmb_size=1024, model_name='try_scheduler_tstcc_pretrain_uci_eps50_lr0.1_bs256_aug1t_warp_aug2negate_dim-pdim128-128_EMA0.996_criterion_NTXent_lambda1_1.0_lambda2_1.0_tempunit_tsfm', n_class=6, n_epoch=50, n_feature=6, p=128, phid=128, plt=False, scheduler=True, split_ratio=0.2, target_domain='0', temp_unit='tsfm', weight_decay=0.0003)

Epoch : 0
Namespace(EMA=0.996, aug1='t_warp', aug2='negate', backbone='ResNet18', batch_size=256, cases='random', criterion='NTXent', cuda=0, dataset='uci', device='Phones', framework='tstcc', lambda1=1.0, lambda2=1.0, len_sw=120, logdir='log/', lr=0.1, lr_cls=0.03, lr_mul=10.0, mmb_size=1024, model_name='try_scheduler_tstcc_pretrain_uci_eps50_lr0.1_bs256_aug1t_warp_aug2negate_dim-pdim128-128_EMA0.996_criterion_NTXent_lambda1_1.0_lambda2_1.0_tempunit_tsfm', n_class=6, n_epoch=50, n_feature=6, p=128, phid=128, plt=False, scheduler=True, split_ratio=0.2, target_domain='0', temp_unit='tsfm', weight_decay=0.0003)
Namespace(EMA=0.996, aug1='t_warp', aug2='negate', backbone='ResNet18', batch_size=256, cases='random', criterion='NTXent', cuda=0, dataset='uci', device='Phones', framework='tstcc', lambda1=1.0, lambda2=1.0, len_sw=128, logdir='log/', lr=0.1, lr_cls=0.03, lr_mul=10.0, mmb_size=1024, model_name='try_scheduler_tstcc_pretrain_uci_eps50_lr0.1_bs256_aug1t_warp_aug2negate_dim-pdim128-128_EMA0.996_criterion_NTXent_lambda1_1.0_lambda2_1.0_tempunit_tsfm', n_class=2, n_epoch=50, n_feature=9, p=128, phid=128, plt=False, scheduler=True, split_ratio=0.2, target_domain='0', temp_unit='tsfm', weight_decay=0.0003)

Epoch : 0
Namespace(EMA=0.996, aug1='t_warp', aug2='negate', backbone='ResNet18', batch_size=256, cases='random', criterion='NTXent', cuda=0, dataset='uci', device='Phones', framework='tstcc', lambda1=1.0, lambda2=1.0, len_sw=128, logdir='log/', lr=0.1, lr_cls=0.03, lr_mul=10.0, mmb_size=1024, model_name='try_scheduler_tstcc_pretrain_uci_eps50_lr0.1_bs256_aug1t_warp_aug2negate_dim-pdim128-128_EMA0.996_criterion_NTXent_lambda1_1.0_lambda2_1.0_tempunit_tsfm', n_class=2, n_epoch=50, n_feature=9, p=128, phid=128, plt=False, scheduler=True, split_ratio=0.2, target_domain='0', temp_unit='tsfm', weight_decay=0.0003)

Epoch : 0
Namespace(EMA=0.996, aug1='t_warp', aug2='negate', backbone='ResNet18', batch_size=256, cases='random', criterion='NTXent', cuda=0, dataset='uci', device='Phones', framework='tstcc', lambda1=1.0, lambda2=1.0, len_sw=128, logdir='log/', lr=0.1, lr_cls=0.03, lr_mul=10.0, mmb_size=1024, model_name='try_scheduler_tstcc_pretrain_uci_eps50_lr0.1_bs256_aug1t_warp_aug2negate_dim-pdim128-128_EMA0.996_criterion_NTXent_lambda1_1.0_lambda2_1.0_tempunit_tsfm', n_class=2, n_epoch=50, n_feature=6, p=128, phid=128, plt=False, scheduler=True, split_ratio=0.2, target_domain='0', temp_unit='tsfm', weight_decay=0.0003)

Epoch : 0
Train Loss : 1207.0363
Val Loss : 0.0000

Epoch : 1
Train Loss : 3308.3406
Val Loss : 0.0000

Epoch : 2
Train Loss : 3367.4171
Val Loss : 0.0000

Epoch : 3
Train Loss : 2488.4370
Val Loss : 0.0000

Epoch : 4
Train Loss : 2775.3335
Val Loss : 0.0000

Epoch : 5
Train Loss : 1959.7712
Val Loss : 0.0000

Epoch : 6
Train Loss : 1161.7282
Val Loss : 0.0000

Epoch : 7
Train Loss : 786.3615
Val Loss : 0.0000

Epoch : 8
Train Loss : 572.4850
Val Loss : 0.0000

Epoch : 9
Train Loss : 369.1939
Val Loss : 0.0000

Epoch : 10
Train Loss : 245.6428
Val Loss : 0.0000

Epoch : 11
Train Loss : 174.4449
Val Loss : 0.0000

Epoch : 12
Train Loss : 107.8771
Val Loss : 0.0000

Epoch : 13
Train Loss : 80.7280
Val Loss : 0.0000

Epoch : 14
Train Loss : 80.9683
Val Loss : 0.0000

Epoch : 15
Train Loss : 144.8116
Val Loss : 0.0000

Epoch : 16
Train Loss : 171.0294
Val Loss : 0.0000

Epoch : 17
Train Loss : 171.6747
Val Loss : 0.0000

Epoch : 18
Train Loss : 130.2597
Val Loss : 0.0000

Epoch : 19
Train Loss : 393.1193
Val Loss : 0.0000

Epoch : 20
Train Loss : 182.8564
Val Loss : 0.0000

Epoch : 21
Train Loss : 138.9592
Val Loss : 0.0000

Epoch : 22
Train Loss : 302.7166
Val Loss : 0.0000

Epoch : 23
Train Loss : 262.9004
Val Loss : 0.0000

Epoch : 24
Train Loss : 169.2080
Val Loss : 0.0000

Epoch : 25
Train Loss : 129.0339
Val Loss : 0.0000

Epoch : 26
Train Loss : 110.2146
Val Loss : 0.0000

Epoch : 27
Train Loss : 70.1846
Val Loss : 0.0000

Epoch : 28
Train Loss : 67.3475
Val Loss : 0.0000

Epoch : 29
Train Loss : 104.4532
Val Loss : 0.0000

Epoch : 30
Train Loss : 87.8219
Val Loss : 0.0000

Epoch : 31
Train Loss : 65.7867
Val Loss : 0.0000

Epoch : 32
Train Loss : 50.6592
Val Loss : 0.0000

Epoch : 33
Train Loss : 39.8514
Val Loss : 0.0000

Epoch : 34
Train Loss : 32.8024
Val Loss : 0.0000

Epoch : 35
Train Loss : 29.5272
Val Loss : 0.0000

Epoch : 36
Train Loss : 25.5273
Val Loss : 0.0000

Epoch : 37
Train Loss : 24.2488
Val Loss : 0.0000

Epoch : 38
Train Loss : 23.0565
Val Loss : 0.0000

Epoch : 39
Train Loss : 22.7697
Val Loss : 0.0000

Epoch : 40
Train Loss : 21.8564
Val Loss : 0.0000

Epoch : 41
Train Loss : 21.1262
Val Loss : 0.0000

Epoch : 42
Train Loss : 21.2677
Val Loss : 0.0000

Epoch : 43
Train Loss : 21.0992
Val Loss : 0.0000

Epoch : 44
Train Loss : 20.6201
Val Loss : 0.0000

Epoch : 45
Train Loss : 20.8936
Val Loss : 0.0000

Epoch : 46
Train Loss : 20.5995
Val Loss : 0.0000

Epoch : 47
Train Loss : 20.5955
Val Loss : 0.0000

Epoch : 48
Train Loss : 20.5061
Val Loss : 0.0000

Epoch : 49
Train Loss : 20.7554
Val Loss : 0.0000
